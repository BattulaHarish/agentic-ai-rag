{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db233ba",
   "metadata": {},
   "source": [
    "### Implementation of RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "870bf000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca4ab6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bbdd8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangGraphDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/quickstart', 'title': 'Quickstart - Docs by LangChain', 'language': 'en'}, page_content='Quickstart - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedQuickstartLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeGet startedQuickstartCopy pageCopy pageThis quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.\\n\\nUse the Graph API if you prefer to define your agent as a graph of nodes and edges.\\nUse the Functional API if you prefer to define your agent as a single function.\\n\\nFor conceptual information, see Graph API overview and Functional API overview.\\nFor this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the ANTHROPIC_API_KEY environment variable in your terminal.\\n Use the Graph API Use the Functional API\\u200b1. Define tools and modelIn this example, weâ€™ll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.Copyfrom langchain.tools import tool\\nfrom langchain.chat_models import init_chat_model\\n\\n\\nmodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    temperature=0\\n)\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nmodel_with_tools = model.bind_tools(tools)\\n\\u200b2. Define stateThe graphâ€™s state is used to store the messages and the number of LLM calls.State in LangGraph persists throughout the agentâ€™s execution.The Annotated type with operator.add ensures that new messages are appended to the existing list rather than replacing it.Copyfrom langchain.messages import AnyMessage\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\n\\nclass MessagesState(TypedDict):\\n    messages: Annotated[list[AnyMessage], operator.add]\\n    llm_calls: int\\n\\u200b3. Define model nodeThe model node is used to call the LLM and decide whether to call a tool or not.Copyfrom langchain.messages import SystemMessage\\n\\n\\ndef llm_call(state: dict):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            model_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ],\\n        \"llm_calls\": state.get(\\'llm_calls\\', 0) + 1\\n    }\\n\\u200b4. Define tool nodeThe tool node is used to call the tools and return the results.Copyfrom langchain.messages import ToolMessage\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\u200b5. Define end logicThe conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.Copyfrom typing import Literal\\nfrom langgraph.graph import StateGraph, START, END\\n\\n\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\u200b6. Build and compile the agentThe agent is built using the StateGraph class and compiled using the compile method.Copy# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\nfrom IPython.display import Image, display\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nfrom langchain.messages import HumanMessage\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\nTo learn how to trace your agent with LangSmith, see the LangSmith documentation.Congratulations! Youâ€™ve built your first agent using the LangGraph Graph API.Full code exampleCopy# Step 1: Define tools and model\\n\\nfrom langchain.tools import tool\\nfrom langchain.chat_models import init_chat_model\\n\\n\\nmodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    temperature=0\\n)\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nmodel_with_tools = model.bind_tools(tools)\\n\\n# Step 2: Define state\\n\\nfrom langchain.messages import AnyMessage\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\n\\nclass MessagesState(TypedDict):\\n    messages: Annotated[list[AnyMessage], operator.add]\\n    llm_calls: int\\n\\n# Step 3: Define model node\\nfrom langchain.messages import SystemMessage\\n\\n\\ndef llm_call(state: dict):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            model_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ],\\n        \"llm_calls\": state.get(\\'llm_calls\\', 0) + 1\\n    }\\n\\n\\n# Step 4: Define tool node\\n\\nfrom langchain.messages import ToolMessage\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n# Step 5: Define logic to determine whether to end\\n\\nfrom typing import Literal\\nfrom langgraph.graph import StateGraph, START, END\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n# Step 6: Build agent\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n\\nfrom IPython.display import Image, display\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nfrom langchain.messages import HumanMessage\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphPreviousRun a local serverNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Streaming - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSupported stream modesBasic usage exampleStream multiple modesStream graph stateStream subgraph outputsDebuggingLLM tokensFilter by LLM invocationFilter by nodeStream custom dataUse with any LLMDisable streaming for specific chat modelsAsync with Python < 3.11CapabilitiesStreamingCopy pageCopy pageLangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\nWhatâ€™s possible with LangGraph streaming:\\n\\n Stream graph state â€” get state updates / values with updates and values modes.\\n Stream subgraph outputs â€” include outputs from both the parent graph and any nested subgraphs.\\n Stream LLM tokens â€” capture token streams from anywhere: inside nodes, subgraphs, or tools.\\n Stream custom data â€” send custom updates or progress signals directly from tool functions.\\n Use multiple streaming modes â€” choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces).\\n\\n\\u200bSupported stream modes\\nPass one or more of the following stream modes as a list to the stream or astream methods:\\nModeDescriptionvaluesStreams the full value of the state after each step of the graph.updatesStreams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.customStreams custom data from inside your graph nodes.messagesStreams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.debugStreams as much information as possible throughout the execution of the graph.\\n\\u200bBasic usage example\\nLangGraph graphs expose the stream (sync) and astream (async) methods to yield streamed outputs as iterators.\\nCopyfor chunk in graph.stream(inputs, stream_mode=\"updates\"):\\n    print(chunk)\\n\\nExtended example: streaming updatesCopyfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n\\ndef refine_topic(state: State):\\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\\n\\ndef generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(refine_topic)\\n    .add_node(generate_joke)\\n    .add_edge(START, \"refine_topic\")\\n    .add_edge(\"refine_topic\", \"generate_joke\")\\n    .add_edge(\"generate_joke\", END)\\n    .compile()\\n)\\n\\n# The stream() method returns an iterator that yields streamed outputs\\nfor chunk in graph.stream(  \\n    {\"topic\": \"ice cream\"},\\n    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\\n    # Other stream modes are also available. See supported stream modes for details\\n    stream_mode=\"updates\",  \\n):\\n    print(chunk)\\nCopy{\\'refineTopic\\': {\\'topic\\': \\'ice cream and cats\\'}}\\n{\\'generateJoke\\': {\\'joke\\': \\'This is a joke about ice cream and cats\\'}}\\n\\n\\u200bStream multiple modes\\nYou can pass a list as the stream_mode parameter to stream multiple modes at once.\\nThe streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.\\nCopyfor mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\\n    print(chunk)\\n\\n\\u200bStream graph state\\nUse the stream modes updates and values to stream the state of the graph as it executes.\\n\\nupdates streams the updates to the state after each step of the graph.\\nvalues streams the full value of the state after each step of the graph.\\n\\nCopyfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\n\\nclass State(TypedDict):\\n  topic: str\\n  joke: str\\n\\n\\ndef refine_topic(state: State):\\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\\n\\n\\ndef generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n  StateGraph(State)\\n  .add_node(refine_topic)\\n  .add_node(generate_joke)\\n  .add_edge(START, \"refine_topic\")\\n  .add_edge(\"refine_topic\", \"generate_joke\")\\n  .add_edge(\"generate_joke\", END)\\n  .compile()\\n)\\n\\n updates valuesUse this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.Copyfor chunk in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"updates\",  \\n):\\n    print(chunk)\\n\\n\\u200bStream subgraph outputs\\nTo include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\\nThe outputs will be streamed as tuples (namespace, data), where namespace is a tuple with the path to the node where a subgraph is invoked, e.g. (\"parent_node:<task_id>\", \"child_node:<task_id>\").\\nCopyfor chunk in graph.stream(\\n    {\"foo\": \"foo\"},\\n    # Set subgraphs=True to stream outputs from subgraphs\\n    subgraphs=True,  \\n    stream_mode=\"updates\",\\n):\\n    print(chunk)\\n\\nExtended example: streaming from subgraphsCopyfrom langgraph.graph import START, StateGraph\\nfrom typing import TypedDict\\n\\n# Define subgraph\\nclass SubgraphState(TypedDict):\\n    foo: str  # note that this key is shared with the parent graph state\\n    bar: str\\n\\ndef subgraph_node_1(state: SubgraphState):\\n    return {\"bar\": \"bar\"}\\n\\ndef subgraph_node_2(state: SubgraphState):\\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\\n\\nsubgraph_builder = StateGraph(SubgraphState)\\nsubgraph_builder.add_node(subgraph_node_1)\\nsubgraph_builder.add_node(subgraph_node_2)\\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\\nsubgraph = subgraph_builder.compile()\\n\\n# Define parent graph\\nclass ParentState(TypedDict):\\n    foo: str\\n\\ndef node_1(state: ParentState):\\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\\n\\nbuilder = StateGraph(ParentState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", subgraph)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\ngraph = builder.compile()\\n\\nfor chunk in graph.stream(\\n    {\"foo\": \"foo\"},\\n    stream_mode=\"updates\",\\n    # Set subgraphs=True to stream outputs from subgraphs\\n    subgraphs=True,  \\n):\\n    print(chunk)\\nCopy((), {\\'node_1\\': {\\'foo\\': \\'hi! foo\\'}})\\n((\\'node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2\\',), {\\'subgraph_node_1\\': {\\'bar\\': \\'bar\\'}})\\n((\\'node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2\\',), {\\'subgraph_node_2\\': {\\'foo\\': \\'hi! foobar\\'}})\\n((), {\\'node_2\\': {\\'foo\\': \\'hi! foobar\\'}})\\nNote that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\\n\\n\\u200bDebugging\\nUse the debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\\nCopyfor chunk in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"debug\",  \\n):\\n    print(chunk)\\n\\n\\n\\u200bLLM tokens\\nUse the messages streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\\nThe streamed output from messages mode is a tuple (message_chunk, metadata) where:\\n\\nmessage_chunk: the token or message segment from the LLM.\\nmetadata: a dictionary containing details about the graph node and LLM invocation.\\n\\n\\nIf your LLM is not available as a LangChain integration, you can stream its outputs using custom mode instead. See use with any LLM for details.\\n\\nManual config required for async in Python < 3.11\\nWhen using Python < 3.11 with async code, you must explicitly pass RunnableConfig to ainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.\\nCopyfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import StateGraph, START\\n\\n\\n@dataclass\\nclass MyState:\\n    topic: str\\n    joke: str = \"\"\\n\\n\\nmodel = init_chat_model(model=\"gpt-4o-mini\")\\n\\ndef call_model(state: MyState):\\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\\n    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\\n    model_response = model.invoke(  \\n        [\\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\\n        ]\\n    )\\n    return {\"joke\": model_response.content}\\n\\ngraph = (\\n    StateGraph(MyState)\\n    .add_node(call_model)\\n    .add_edge(START, \"call_model\")\\n    .compile()\\n)\\n\\n# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor message_chunk, metadata in graph.stream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"messages\",  \\n):\\n    if message_chunk.content:\\n        print(message_chunk.content, end=\"|\", flush=True)\\n\\n\\u200bFilter by LLM invocation\\nYou can associate tags with LLM invocations to filter the streamed tokens by LLM invocation.\\nCopyfrom langchain.chat_models import init_chat_model\\n\\n# model_1 is tagged with \"joke\"\\nmodel_1 = init_chat_model(model=\"gpt-4o-mini\", tags=[\\'joke\\'])\\n# model_2 is tagged with \"poem\"\\nmodel_2 = init_chat_model(model=\"gpt-4o-mini\", tags=[\\'poem\\'])\\n\\ngraph = ... # define a graph that uses these LLMs\\n\\n# The stream_mode is set to \"messages\" to stream LLM tokens\\n# The metadata contains information about the LLM invocation, including the tags\\nasync for msg, metadata in graph.astream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the tags field in the metadata to only include\\n    # the tokens from the LLM invocation with the \"joke\" tag\\n    if metadata[\"tags\"] == [\"joke\"]:\\n        print(msg.content, end=\"|\", flush=True)\\n\\nExtended example: filtering by tagsCopyfrom typing import TypedDict\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import START, StateGraph\\n\\n# The joke_model is tagged with \"joke\"\\njoke_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"joke\"])\\n# The poem_model is tagged with \"poem\"\\npoem_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"poem\"])\\n\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n      poem: str\\n\\n\\nasync def call_model(state, config):\\n      topic = state[\"topic\"]\\n      print(\"Writing joke...\")\\n      # Note: Passing the config through explicitly is required for python < 3.11\\n      # Since context var support wasn\\'t added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\\n      # The config is passed through explicitly to ensure the context vars are propagated correctly\\n      # This is required for Python < 3.11 when using async code. Please see the async section for more details\\n      joke_response = await joke_model.ainvoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\\n            config,\\n      )\\n      print(\"\\\\n\\\\nWriting poem...\")\\n      poem_response = await poem_model.ainvoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\\n            config,\\n      )\\n      return {\"joke\": joke_response.content, \"poem\": poem_response.content}\\n\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(call_model)\\n      .add_edge(START, \"call_model\")\\n      .compile()\\n)\\n\\n# The stream_mode is set to \"messages\" to stream LLM tokens\\n# The metadata contains information about the LLM invocation, including the tags\\nasync for msg, metadata in graph.astream(\\n      {\"topic\": \"cats\"},\\n      stream_mode=\"messages\",\\n):\\n    if metadata[\"tags\"] == [\"joke\"]:\\n        print(msg.content, end=\"|\", flush=True)\\n\\n\\u200bFilter by node\\nTo stream tokens only from specific nodes, use stream_mode=\"messages\" and filter the outputs by the langgraph_node field in the streamed metadata:\\nCopy# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor msg, metadata in graph.stream(\\n    inputs,\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the langgraph_node field in the metadata\\n    # to only include the tokens from the specified node\\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\\n        ...\\n\\nExtended example: streaming LLM tokens from specific nodesCopyfrom typing import TypedDict\\nfrom langgraph.graph import START, StateGraph\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\n\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n      poem: str\\n\\n\\ndef write_joke(state: State):\\n      topic = state[\"topic\"]\\n      joke_response = model.invoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\\n      )\\n      return {\"joke\": joke_response.content}\\n\\n\\ndef write_poem(state: State):\\n      topic = state[\"topic\"]\\n      poem_response = model.invoke(\\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\\n      )\\n      return {\"poem\": poem_response.content}\\n\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(write_joke)\\n      .add_node(write_poem)\\n      # write both the joke and the poem concurrently\\n      .add_edge(START, \"write_joke\")\\n      .add_edge(START, \"write_poem\")\\n      .compile()\\n)\\n\\n# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\\n# with information about the graph node where the LLM was called and other information\\nfor msg, metadata in graph.stream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"messages\",  \\n):\\n    # Filter the streamed tokens by the langgraph_node field in the metadata\\n    # to only include the tokens from the write_poem node\\n    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\\n        print(msg.content, end=\"|\", flush=True)\\n\\n\\u200bStream custom data\\nTo send custom user-defined data from inside a LangGraph node or tool, follow these steps:\\n\\nUse get_stream_writer to access the stream writer and emit custom data.\\nSet stream_mode=\"custom\" when calling .stream() or .astream() to get the custom data in the stream. You can combine multiple modes (e.g., [\"updates\", \"custom\"]), but at least one must be \"custom\".\\n\\nNo get_stream_writer in async for Python < 3.11\\nIn async code running on Python < 3.11, get_stream_writer will not work.\\nInstead, add a writer parameter to your node or tool and pass it manually.\\nSee Async with Python < 3.11 for usage examples.\\n node toolCopyfrom typing import TypedDict\\nfrom langgraph.config import get_stream_writer\\nfrom langgraph.graph import StateGraph, START\\n\\nclass State(TypedDict):\\n    query: str\\n    answer: str\\n\\ndef node(state: State):\\n    # Get the stream writer to send custom data\\n    writer = get_stream_writer()\\n    # Emit a custom key-value pair (e.g., progress update)\\n    writer({\"custom_key\": \"Generating custom data inside node\"})\\n    return {\"answer\": \"some data\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(node)\\n    .add_edge(START, \"node\")\\n    .compile()\\n)\\n\\ninputs = {\"query\": \"example\"}\\n\\n# Set stream_mode=\"custom\" to receive the custom data in the stream\\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"):\\n    print(chunk)\\n\\n\\u200bUse with any LLM\\nYou can use stream_mode=\"custom\" to stream data from any LLM API â€” even if that API does not implement the LangChain chat model interface.\\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\\nCopyfrom langgraph.config import get_stream_writer\\n\\ndef call_arbitrary_model(state):\\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\\n    # Get the stream writer to send custom data\\n    writer = get_stream_writer()  \\n    # Assume you have a streaming client that yields chunks\\n    # Generate LLM tokens using your custom streaming client\\n    for chunk in your_custom_streaming_client(state[\"topic\"]):\\n        # Use the writer to send custom data to the stream\\n        writer({\"custom_llm_chunk\": chunk})  \\n    return {\"result\": \"completed\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(call_arbitrary_model)\\n    # Add other nodes and edges as needed\\n    .compile()\\n)\\n# Set stream_mode=\"custom\" to receive the custom data in the stream\\nfor chunk in graph.stream(\\n    {\"topic\": \"cats\"},\\n    stream_mode=\"custom\",  \\n\\n):\\n    # The chunk will contain the custom data streamed from the llm\\n    print(chunk)\\n\\nExtended example: streaming arbitrary chat modelCopyimport operator\\nimport json\\n\\nfrom typing import TypedDict\\nfrom typing_extensions import Annotated\\nfrom langgraph.graph import StateGraph, START\\n\\nfrom openai import AsyncOpenAI\\n\\nopenai_client = AsyncOpenAI()\\nmodel_name = \"gpt-4o-mini\"\\n\\n\\nasync def stream_tokens(model_name: str, messages: list[dict]):\\n    response = await openai_client.chat.completions.create(\\n        messages=messages, model=model_name, stream=True\\n    )\\n    role = None\\n    async for chunk in response:\\n        delta = chunk.choices[0].delta\\n\\n        if delta.role is not None:\\n            role = delta.role\\n\\n        if delta.content:\\n            yield {\"role\": role, \"content\": delta.content}\\n\\n\\n# this is our tool\\nasync def get_items(place: str) -> str:\\n    \"\"\"Use this tool to list items one might find in a place you\\'re asked about.\"\"\"\\n    writer = get_stream_writer()\\n    response = \"\"\\n    async for msg_chunk in stream_tokens(\\n        model_name,\\n        [\\n            {\\n                \"role\": \"user\",\\n                \"content\": (\\n                    \"Can you tell me what kind of items \"\\n                    f\"i might find in the following place: \\'{place}\\'. \"\\n                    \"List at least 3 such items separating them by a comma. \"\\n                    \"And include a brief description of each item.\"\\n                ),\\n            }\\n        ],\\n    ):\\n        response += msg_chunk[\"content\"]\\n        writer(msg_chunk)\\n\\n    return response\\n\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[dict], operator.add]\\n\\n\\n# this is the tool-calling graph node\\nasync def call_tool(state: State):\\n    ai_message = state[\"messages\"][-1]\\n    tool_call = ai_message[\"tool_calls\"][-1]\\n\\n    function_name = tool_call[\"function\"][\"name\"]\\n    if function_name != \"get_items\":\\n        raise ValueError(f\"Tool {function_name} not supported\")\\n\\n    function_arguments = tool_call[\"function\"][\"arguments\"]\\n    arguments = json.loads(function_arguments)\\n\\n    function_response = await get_items(**arguments)\\n    tool_message = {\\n        \"tool_call_id\": tool_call[\"id\"],\\n        \"role\": \"tool\",\\n        \"name\": function_name,\\n        \"content\": function_response,\\n    }\\n    return {\"messages\": [tool_message]}\\n\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(call_tool)\\n    .add_edge(START, \"call_tool\")\\n    .compile()\\n)\\nLetâ€™s invoke the graph with an AIMessage that includes a tool call:Copyinputs = {\\n    \"messages\": [\\n        {\\n            \"content\": None,\\n            \"role\": \"assistant\",\\n            \"tool_calls\": [\\n                {\\n                    \"id\": \"1\",\\n                    \"function\": {\\n                        \"arguments\": \\'{\"place\":\"bedroom\"}\\',\\n                        \"name\": \"get_items\",\\n                    },\\n                    \"type\": \"function\",\\n                }\\n            ],\\n        }\\n    ]\\n}\\n\\nasync for chunk in graph.astream(\\n    inputs,\\n    stream_mode=\"custom\",\\n):\\n    print(chunk[\"content\"], end=\"|\", flush=True)\\n\\n\\u200bDisable streaming for specific chat models\\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\\nmodels that do not support it.\\nSet disable_streaming=True when initializing the model.\\n init_chat_model Chat model interfaceCopyfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    # Set disable_streaming=True to disable streaming for the chat model\\n    disable_streaming=True\\n\\n)\\n\\n\\n\\u200bAsync with Python < 3.11\\nIn Python versions < 3.11, asyncio tasks do not support the context parameter.\\nThis limits LangGraph ability to automatically propagate context, and affects LangGraphâ€™s streaming mechanisms in two key ways:\\n\\nYou must explicitly pass RunnableConfig into async LLM calls (e.g., ainvoke()), as callbacks are not automatically propagated.\\nYou cannot use get_stream_writer in async nodes or tools â€” you must pass a writer argument directly.\\n\\nExtended example: async LLM call with manual configCopyfrom typing import TypedDict\\nfrom langgraph.graph import START, StateGraph\\nfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(model=\"gpt-4o-mini\")\\n\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n\\n# Accept config as an argument in the async node function\\nasync def call_model(state, config):\\n    topic = state[\"topic\"]\\n    print(\"Generating joke...\")\\n    # Pass config to model.ainvoke() to ensure proper context propagation\\n    joke_response = await model.ainvoke(  \\n        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\\n        config,\\n    )\\n    return {\"joke\": joke_response.content}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(call_model)\\n    .add_edge(START, \"call_model\")\\n    .compile()\\n)\\n\\n# Set stream_mode=\"messages\" to stream LLM tokens\\nasync for chunk, metadata in graph.astream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"messages\",  \\n):\\n    if chunk.content:\\n        print(chunk.content, end=\"|\", flush=True)\\n\\nExtended example: async custom streaming with stream writerCopyfrom typing import TypedDict\\nfrom langgraph.types import StreamWriter\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n\\n# Add writer as an argument in the function signature of the async node or tool\\n# LangGraph will automatically pass the stream writer to the function\\nasync def generate_joke(state: State, writer: StreamWriter):  \\n      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\\n      return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(generate_joke)\\n      .add_edge(START, \"generate_joke\")\\n      .compile()\\n)\\n\\n# Set stream_mode=\"custom\" to receive the custom data in the stream  #\\nasync for chunk in graph.astream(\\n      {\"topic\": \"ice cream\"},\\n      stream_mode=\"custom\",\\n):\\n      print(chunk)\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoDurable executionPreviousInterruptsNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/quickstart\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/streaming\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b412153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangGraphDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]\n"
     ]
    }
   ],
   "source": [
    "doc_list=[item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "doc_split=text_splitter.split_documents(doc_list)\n",
    "print(doc_split[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55d966b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ae0049a9-9411-4193-b56e-8d94acff431a', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangGraphDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(id='d7246540-6bdb-409a-91e0-08cc3acfefb8', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agentsâ€” including Klarna, Replit, Elastic, and moreâ€” LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you donâ€™t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChainâ€™s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.'),\n",
       " Document(id='e99a1572-d7ca-4b1f-a268-3b9b8a8851b0', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='Streaming - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSupported stream modesBasic usage exampleStream multiple modesStream graph stateStream subgraph outputsDebuggingLLM tokensFilter by LLM invocationFilter by nodeStream custom dataUse with any LLMDisable streaming for specific chat modelsAsync with Python < 3.11CapabilitiesStreamingCopy pageCopy pageLangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\nWhatâ€™s possible with LangGraph streaming:\\n\\n Stream graph state â€” get state updates / values with updates and values modes.\\n Stream subgraph outputs â€” include outputs from both the parent graph and any nested subgraphs.\\n Stream LLM tokens â€” capture token streams from anywhere: inside nodes, subgraphs, or tools.\\n Stream custom data â€” send custom updates or progress signals directly from tool functions.\\n Use multiple streaming modes â€” choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces).\\n\\n\\u200bSupported stream modes\\nPass one or more of the following stream modes as a list to the stream or astream methods:\\nModeDescriptionvaluesStreams the full value of the state after each step of the graph.updatesStreams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.customStreams custom data from inside your graph nodes.messagesStreams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.debugStreams as much information as possible throughout the execution of the graph.\\n\\u200bBasic usage example\\nLangGraph graphs expose the stream (sync) and astream (async) methods to yield streamed outputs as iterators.\\nCopyfor chunk in graph.stream(inputs, stream_mode=\"updates\"):\\n    print(chunk)\\n\\nExtended example: streaming updatesCopyfrom typing import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n\\ndef refine_topic(state: State):\\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\\n\\ndef generate_joke(state: State):\\n    return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n    StateGraph(State)\\n    .add_node(refine_topic)\\n    .add_node(generate_joke)\\n    .add_edge(START, \"refine_topic\")\\n    .add_edge(\"refine_topic\", \"generate_joke\")\\n    .add_edge(\"generate_joke\", END)\\n    .compile()\\n)\\n\\n# The stream() method returns an iterator that yields streamed outputs\\nfor chunk in graph.stream(  \\n    {\"topic\": \"ice cream\"},\\n    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\\n    # Other stream modes are also available. See supported stream modes for details\\n    stream_mode=\"updates\",  \\n):\\n    print(chunk)\\nCopy{\\'refineTopic\\': {\\'topic\\': \\'ice cream and cats\\'}}\\n{\\'generateJoke\\': {\\'joke\\': \\'This is a joke about ice cream and cats\\'}}'),\n",
       " Document(id='c2366b70-56bf-45d8-856b-0d006a219afa', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/streaming', 'title': 'Streaming - Docs by LangChain', 'language': 'en'}, page_content='graph = (\\n    StateGraph(State)\\n    .add_node(call_model)\\n    .add_edge(START, \"call_model\")\\n    .compile()\\n)\\n\\n# Set stream_mode=\"messages\" to stream LLM tokens\\nasync for chunk, metadata in graph.astream(\\n    {\"topic\": \"ice cream\"},\\n    stream_mode=\"messages\",  \\n):\\n    if chunk.content:\\n        print(chunk.content, end=\"|\", flush=True)\\n\\nExtended example: async custom streaming with stream writerCopyfrom typing import TypedDict\\nfrom langgraph.types import StreamWriter\\n\\nclass State(TypedDict):\\n      topic: str\\n      joke: str\\n\\n# Add writer as an argument in the function signature of the async node or tool\\n# LangGraph will automatically pass the stream writer to the function\\nasync def generate_joke(state: State, writer: StreamWriter):  \\n      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\\n      return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n      StateGraph(State)\\n      .add_node(generate_joke)\\n      .add_edge(START, \"generate_joke\")\\n      .compile()\\n)\\n\\n# Set stream_mode=\"custom\" to receive the custom data in the stream  #\\nasync for chunk in graph.astream(\\n      {\"topic\": \"ice cream\"},\\n      stream_mode=\"custom\",\\n):\\n      print(chunk)\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoDurable executionPreviousInterruptsNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Add all these text to vectorDB\n",
    "vectorstore=FAISS.from_documents(documents=doc_split,embedding=OpenAIEmbeddings())\n",
    "retriever=vectorstore.as_retriever()\n",
    "retriever.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c8d1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever to Retriever Tools\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search for information about LangGraph. For any questions about LangGraph, you must use this tool!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96fa6684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search for information about LangGraph. For any questions about LangGraph, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000221F55DA050>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C1297DF0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000221C08F37F0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C1297DF0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abd9380d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://docs.langchain.com/', 'title': 'Home - Docs by LangChain', 'language': 'en'}, page_content=\"Home - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageHomeSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationDocumentationLangChain is the platform for agent engineering. AI teams at Replit, Clay, Rippling, Cloudflare, Workday, and more trust LangChainâ€™s products to engineer reliable agents.Open source agent frameworks Python TypeScriptLangChain (Python)Quickly get started building agents, with any model provider of your choice.Learn moreLangGraph (Python)Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.Learn moreDeep Agents (Python)Build agents that can tackle complex, multi-step tasks.Learn moreLangSmithLangSmith is a platform that helps AI teams use live production data for continuous testing and improvement. LangSmith provides:ObservabilitySee exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.Learn moreEvaluationTest and score agent behavior on production data or offline datasets to continuously improve performance.Learn morePrompt EngineeringIterate on prompts with version control, prompt optimization, and collaboration features.Learn moreDeploymentShip your agent in one click, using scalable infrastructure built for long-running tasks.Learn moreAgent Builder (Beta)Turn natural-language ideas into production agents with persistent memory and self-updating capabilities.Learn moreLangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.Get startedBuild your first agent with LangChainGet startedSign up for LangSmithTry LangSmithBuild an advanced agent with LangGraphGet startedEnroll in LangChain AcademyGet startedAdditional resourcesCommunity forumAsk questions, share solutions, and discuss best practices.JoinCommunity SlackConnect with other builders and get quick help.JoinSupport portalSubmit tickets and track support requests.VisitLangSmith statusReal-time status of LangSmith services and APIs.View\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\")],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Create an agent\\nCopy# pip install -qU langchain \"langchain[anthropic]\"\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nSee the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChainâ€™s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChainâ€™s agents are built on top of LangGraph. This allows us to take advantage of LangGraphâ€™s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangChainNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/models', 'title': 'Models - Docs by LangChain', 'language': 'en'}, page_content='Models - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsModelsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageBasic usageInitialize a modelKey methodsParametersInvocationInvokeStreamBatchTool callingStructured outputSupported modelsAdvanced topicsModel profilesMultimodalReasoningLocal modelsPrompt cachingServer-side tool useRate limitingBase URL or proxyLog probabilitiesToken usageInvocation configConfigurable modelsCore componentsModelsCopy pageCopy pageLLMs are powerful AI tools that can interpret and generate text like humans. Theyâ€™re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\\nIn addition to text generation, many models support:\\n\\n Tool calling - calling external tools (like databases queries or API calls) and use results in their responses.\\n Structured output - where the modelâ€™s response is constrained to follow a defined format.\\n Multimodality - process and return data other than text, such as images, audio, and video.\\n Reasoning - models perform multi-step reasoning to arrive at a conclusion.\\n\\nModels are the reasoning engine of agents. They drive the agentâ€™s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\\nThe quality and capabilities of the model you choose directly impact your agentâ€™s baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\\nLangChainâ€™s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\\nFor provider-specific integration information and capabilities, see the providerâ€™s chat model page.\\n\\u200bBasic usage\\nModels can be utilized in two ways:\\n\\nWith agents - Models can be dynamically specified when creating an agent.\\nStandalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\\n\\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\\n\\u200bInitialize a model\\nThe easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below):\\n OpenAI Anthropic Azure Google Gemini AWS BedrockðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nCopyresponse = model.invoke(\"Why do parrots talk?\")\\n\\nSee init_chat_model for more detail, including information on how to pass model parameters.\\n\\u200bKey methods\\nInvokeThe model takes messages as input and outputs messages after generating a complete response.\\nStreamInvoke the model, but stream the output as it is generated in real-time.\\nBatchSend multiple requests to a model in a batch for more efficient processing.\\nIn addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details.\\n\\u200bParameters\\nA chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\\n\\u200bmodelstringrequiredThe name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the â€™:â€™ format, for example, â€˜openai:o1â€™.\\n\\u200bapi_keystringThe key required for authenticating with the modelâ€™s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable.\\n\\u200btemperaturenumberControls the randomness of the modelâ€™s output. A higher number makes responses more creative; lower ones make them more deterministic.\\n\\u200bmax_tokensnumberLimits the total number of tokens in the response, effectively controlling how long the output can be.\\n\\u200btimeoutnumberThe maximum time (in seconds) to wait for a response from the model before canceling the request.\\n\\u200bmax_retriesnumberThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\\nUsing init_chat_model, pass these parameters as inline **kwargs:\\nInitialize using model parametersCopymodel = init_chat_model(\\n    \"claude-sonnet-4-5-20250929\",\\n    # Kwargs passed to the model:\\n    temperature=0.7,\\n    timeout=30,\\n    max_tokens=1000,\\n)\\n\\nEach chat model integration may have additional params used to control provider-specific functionality.For example, ChatOpenAI has use_responses_api to dictate whether to use the OpenAI Responses or Completions API.To find all the parameters supported by a given chat model, head to the chat model integrations page.\\n\\n\\u200bInvocation\\nA chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.\\n\\u200bInvoke\\nThe most straightforward way to call a model is to use invoke() with a single message or a list of messages.\\nSingle messageCopyresponse = model.invoke(\"Why do parrots have colorful feathers?\")\\nprint(response)\\n\\nA list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\\nSee the messages guide for more detail on roles, types, and content.\\nDictionary formatCopyconversation = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\\n    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\\n    {\"role\": \"assistant\", \"content\": \"J\\'adore la programmation.\"},\\n    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\\n]\\n\\nresponse = model.invoke(conversation)\\nprint(response)  # AIMessage(\"J\\'adore crÃ©er des applications.\")\\n\\nMessage objectsCopyfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\\n\\nconversation = [\\n    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\\n    HumanMessage(\"Translate: I love programming.\"),\\n    AIMessage(\"J\\'adore la programmation.\"),\\n    HumanMessage(\"Translate: I love building applications.\")\\n]\\n\\nresponse = model.invoke(conversation)\\nprint(response)  # AIMessage(\"J\\'adore crÃ©er des applications.\")\\n\\nIf the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with â€œChatâ€, e.g., ChatOpenAI(/oss/integrations/chat/openai).\\n\\u200bStream\\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\\nCalling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\\nBasic text streamingStream tool calls, reasoning, and other contentCopyfor chunk in model.stream(\"Why do parrots have colorful feathers?\"):\\n    print(chunk.text, end=\"|\", flush=True)\\n\\nAs opposed to invoke(), which returns a single AIMessage after the model has finished generating its full response, stream() returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:\\nConstruct an AIMessageCopyfull = None  # None | AIMessageChunk\\nfor chunk in model.stream(\"What color is the sky?\"):\\n    full = chunk if full is None else full + chunk\\n    print(full.text)\\n\\n# The\\n# The sky\\n# The sky is\\n# The sky is typically\\n# The sky is typically blue\\n# ...\\n\\nprint(full.content_blocks)\\n# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\\n\\nThe resulting message can be treated the same as a message that was generated with invoke() â€“ for example, it can be aggregated into a message history and passed back to the model as conversational context.\\nStreaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isnâ€™t streaming-capable would be one that needs to store the entire output in memory before it can be processed.\\nAdvanced streaming topicsStreaming eventsLangChain chat models can also stream semantic events using astream_events().This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.Copyasync for event in model.astream_events(\"Hello\"):\\n\\n    if event[\"event\"] == \"on_chat_model_start\":\\n        print(f\"Input: {event[\\'data\\'][\\'input\\']}\")\\n\\n    elif event[\"event\"] == \"on_chat_model_stream\":\\n        print(f\"Token: {event[\\'data\\'][\\'chunk\\'].text}\")\\n\\n    elif event[\"event\"] == \"on_chat_model_end\":\\n        print(f\"Full message: {event[\\'data\\'][\\'output\\'].text}\")\\n\\n    else:\\n        pass\\nCopyInput: Hello\\nToken: Hi\\nToken:  there\\nToken: !\\nToken:  How\\nToken:  can\\nToken:  I\\n...\\nFull message: Hi there! How can I help today?\\nSee the astream_events() reference for event types and other details.\"Auto-streaming\" chat modelsLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.In LangGraph agents, for example, you can call model.invoke() within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\\u200bHow it worksWhen you invoke() a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking on_llm_new_token events in LangChainâ€™s callback system.Callback events allow LangGraph stream() and astream_events() to surface the chat modelâ€™s output in real-time.\\n\\u200bBatch\\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\\nBatchCopyresponses = model.batch([\\n    \"Why do parrots have colorful feathers?\",\\n    \"How do airplanes fly?\",\\n    \"What is quantum computing?\"\\n])\\nfor response in responses:\\n    print(response)\\n\\nThis section describes a chat model method batch(), which parallelizes model calls client-side.It is distinct from batch APIs supported by inference providers, such as OpenAI or Anthropic.\\nBy default, batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed():\\nYield batch responses upon completionCopyfor response in model.batch_as_completed([\\n    \"Why do parrots have colorful feathers?\",\\n    \"How do airplanes fly?\",\\n    \"What is quantum computing?\"\\n]):\\n    print(response)\\n\\nWhen using batch_as_completed(), results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\\nWhen processing a large number of inputs using batch() or batch_as_completed(), you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary.Batch with max concurrencyCopymodel.batch(\\n    list_of_inputs,\\n    config={\\n        \\'max_concurrency\\': 5,  # Limit to 5 parallel calls\\n    }\\n)\\nSee the RunnableConfig reference for a full list of supported attributes.\\nFor more details on batching, see the reference.\\n\\n\\u200bTool calling\\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\\n\\nA schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\\nA function or coroutine to execute.\\n\\nYou may hear the term â€œfunction callingâ€. We use this interchangeably with â€œtool callingâ€.\\nHereâ€™s the basic tool calling flow between a user and a model:\\n\\nTo make tools that you have defined available for use by a model, you must bind them using bind_tools. In subsequent invocations, the model can choose to call any of the bound tools as needed.\\nSome model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. ChatOpenAI, ChatAnthropic). Check the respective provider reference for details.\\nSee the tools guide for details and other options for creating tools.\\nBinding user toolsCopyfrom langchain.tools import tool\\n\\n@tool\\ndef get_weather(location: str) -> str:\\n    \"\"\"Get the weather at a location.\"\"\"\\n    return f\"It\\'s sunny in {location}.\"\\n\\n\\nmodel_with_tools = model.bind_tools([get_weather])  \\n\\nresponse = model_with_tools.invoke(\"What\\'s the weather like in Boston?\")\\nfor tool_call in response.tool_calls:\\n    # View tool calls made by the model\\n    print(f\"Tool: {tool_call[\\'name\\']}\")\\n    print(f\"Args: {tool_call[\\'args\\']}\")\\n\\nWhen binding user-defined tools, the modelâ€™s response includes a request to execute a tool. When using a model separately from an agent, it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an agent, the agent loop will handle the tool execution loop for you.\\nBelow, we show some common ways you can use tool calling.\\nTool execution loopWhen a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes agent abstractions that handle this orchestration for you.Hereâ€™s a simple example of how to do this:Tool execution loopCopy# Bind (potentially multiple) tools to the model\\nmodel_with_tools = model.bind_tools([get_weather])\\n\\n# Step 1: Model generates tool calls\\nmessages = [{\"role\": \"user\", \"content\": \"What\\'s the weather in Boston?\"}]\\nai_msg = model_with_tools.invoke(messages)\\nmessages.append(ai_msg)\\n\\n# Step 2: Execute tools and collect results\\nfor tool_call in ai_msg.tool_calls:\\n    # Execute the tool with the generated arguments\\n    tool_result = get_weather.invoke(tool_call)\\n    messages.append(tool_result)\\n\\n# Step 3: Pass results back to model for final response\\nfinal_response = model_with_tools.invoke(messages)\\nprint(final_response.text)\\n# \"The current weather in Boston is 72Â°F and sunny.\"\\nEach ToolMessage returned by the tool includes a tool_call_id that matches the original tool call, helping the model correlate results with requests.Forcing tool callsBy default, the model has the freedom to choose which bound tool to use based on the userâ€™s input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or any tool from a given list:Force use of any toolForce use of specific toolsCopymodel_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\\nParallel tool callsMany models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.Parallel tool callsCopymodel_with_tools = model.bind_tools([get_weather])\\n\\nresponse = model_with_tools.invoke(\\n    \"What\\'s the weather in Boston and Tokyo?\"\\n)\\n\\n\\n# The model may generate multiple tool calls\\nprint(response.tool_calls)\\n# [\\n#   {\\'name\\': \\'get_weather\\', \\'args\\': {\\'location\\': \\'Boston\\'}, \\'id\\': \\'call_1\\'},\\n#   {\\'name\\': \\'get_weather\\', \\'args\\': {\\'location\\': \\'Tokyo\\'}, \\'id\\': \\'call_2\\'},\\n# ]\\n\\n\\n# Execute all tools (can be done in parallel with async)\\nresults = []\\nfor tool_call in response.tool_calls:\\n    if tool_call[\\'name\\'] == \\'get_weather\\':\\n        result = get_weather.invoke(tool_call)\\n    ...\\n    results.append(result)\\nThe model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.Most models supporting tool calling enable parallel tool calls by default. Some (including OpenAI and Anthropic) allow you to disable this feature. To do this, set parallel_tool_calls=False:Copymodel.bind_tools([get_weather], parallel_tool_calls=False)\\nStreaming tool callsWhen streaming responses, tool calls are progressively built through ToolCallChunk. This allows you to see tool calls as theyâ€™re being generated rather than waiting for the complete response.Streaming tool callsCopyfor chunk in model_with_tools.stream(\\n    \"What\\'s the weather in Boston and Tokyo?\"\\n):\\n    # Tool call chunks arrive progressively\\n    for tool_chunk in chunk.tool_call_chunks:\\n        if name := tool_chunk.get(\"name\"):\\n            print(f\"Tool: {name}\")\\n        if id_ := tool_chunk.get(\"id\"):\\n            print(f\"ID: {id_}\")\\n        if args := tool_chunk.get(\"args\"):\\n            print(f\"Args: {args}\")\\n\\n# Output:\\n# Tool: get_weather\\n# ID: call_SvMlU1TVIZugrFLckFE2ceRE\\n# Args: {\"lo\\n# Args: catio\\n# Args: n\": \"B\\n# Args: osto\\n# Args: n\"}\\n# Tool: get_weather\\n# ID: call_QMZdy6qInx13oWKE7KhuhOLR\\n# Args: {\"lo\\n# Args: catio\\n# Args: n\": \"T\\n# Args: okyo\\n# Args: \"}\\nYou can accumulate chunks to build complete tool calls:Accumulate tool callsCopygathered = None\\nfor chunk in model_with_tools.stream(\"What\\'s the weather in Boston?\"):\\n    gathered = chunk if gathered is None else gathered + chunk\\n    print(gathered.tool_calls)\\n\\n\\n\\u200bStructured output\\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\\n Pydantic TypedDict JSON SchemaPydantic models provide the richest feature set with field validation, descriptions, and nested structures.Copyfrom pydantic import BaseModel, Field\\n\\nclass Movie(BaseModel):\\n    \"\"\"A movie with details.\"\"\"\\n    title: str = Field(..., description=\"The title of the movie\")\\n    year: int = Field(..., description=\"The year the movie was released\")\\n    director: str = Field(..., description=\"The director of the movie\")\\n    rating: float = Field(..., description=\"The movie\\'s rating out of 10\")\\n\\nmodel_with_structure = model.with_structured_output(Movie)\\nresponse = model_with_structure.invoke(\"Provide details about the movie Inception\")\\nprint(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\\n\\nKey considerations for structured output:\\nMethod parameter: Some providers support different methods (\\'json_schema\\', \\'function_calling\\', \\'json_mode\\')\\n\\n\\'json_schema\\' typically refers to dedicated structured output features offered by a provider\\n\\'function_calling\\' derives structured output by forcing a tool call following the given schema\\n\\'json_mode\\' is a precursor to \\'json_schema\\' offered by some providers - it generates valid json, but the schema must be described in the prompt\\n\\n\\nInclude raw: Use include_raw=True to get both the parsed output and the raw AI message\\nValidation: Pydantic models provide automatic validation, while TypedDict and JSON Schema require manual validation\\n\\nExample: Message output alongside parsed structureIt can be useful to return the raw AIMessage object alongside the parsed representation to access response metadata such as token counts. To do this, set include_raw=True when calling with_structured_output:Copyfrom pydantic import BaseModel, Field\\n\\nclass Movie(BaseModel):\\n    \"\"\"A movie with details.\"\"\"\\n    title: str = Field(..., description=\"The title of the movie\")\\n    year: int = Field(..., description=\"The year the movie was released\")\\n    director: str = Field(..., description=\"The director of the movie\")\\n    rating: float = Field(..., description=\"The movie\\'s rating out of 10\")\\n\\nmodel_with_structure = model.with_structured_output(Movie, include_raw=True)  \\nresponse = model_with_structure.invoke(\"Provide details about the movie Inception\")\\nresponse\\n# {\\n#     \"raw\": AIMessage(...),\\n#     \"parsed\": Movie(title=..., year=..., ...),\\n#     \"parsing_error\": None,\\n# }\\n\\nExample: Nested structuresSchemas can be nested:Pydantic BaseModelTypedDictCopyfrom pydantic import BaseModel, Field\\n\\nclass Actor(BaseModel):\\n    name: str\\n    role: str\\n\\nclass MovieDetails(BaseModel):\\n    title: str\\n    year: int\\n    cast: list[Actor]\\n    genres: list[str]\\n    budget: float | None = Field(None, description=\"Budget in millions USD\")\\n\\nmodel_with_structure = model.with_structured_output(MovieDetails)\\n\\n\\n\\u200bSupported models\\nLangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.\\n\\n\\u200bAdvanced topics\\n\\u200bModel profiles\\n This is a beta feature. The format of model profiles is subject to change. \\n Model profiles require langchain>=1.1. \\nLangChain chat models can expose a dictionary of supported features and capabilities through a .profile attribute:\\nCopymodel.profile\\n# {\\n#   \"max_input_tokens\": 400000,\\n#   \"image_inputs\": True,\\n#   \"reasoning_output\": True,\\n#   \"tool_calling\": True,\\n#   ...\\n# }\\n\\nRefer to the full set of fields in the API reference.\\nMuch of the model profile data is powered by the models.dev project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.\\nModel profile data allow applications to work around model capabilities dynamically. For example:\\n\\nSummarization middleware can trigger summarization based on a modelâ€™s context window size.\\nStructured output strategies in create_agent can be inferred automatically (e.g., by checking support for native structured output features).\\nModel inputs can be gated based on supported modalities and maximum input tokens.\\n\\nUpdating or overwriting profile dataModel profile data can be changed if it is missing, stale, or incorrect.Option 1 (quick fix)You can instantiate a chat model with any valid profile:Copycustom_profile = {\\n    \"max_input_tokens\": 100_000,\\n    \"tool_calling\": True,\\n    \"structured_output\": True,\\n    # ...\\n}\\nmodel = init_chat_model(\"...\", profile=custom_profile)\\nThe profile is also a regular dict and can be updated in place. If the model instance is shared, consider using model_copy to avoid mutating shared state.Copynew_profile = model.profile | {\"key\": \"value\"}\\nmodel.model_copy(update={\"profile\": new_profile})\\nOption 2 (fix data upstream)The primary source for the data is the models.dev project. This data is merged with additional fields and overrides in LangChain integration packages and are shipped with those packages.Model profile data can be updated through the following process:\\n(If needed) update the source data at models.dev through a pull request to its repository on Github.\\n(If needed) update additional fields and overrides in langchain_<package>/data/profile_augmentations.toml through a pull request to the LangChain integration package`.\\nUse the langchain-model-profiles CLI tool to pull the latest data from models.dev, merge in the augmentations and update the profile data:\\nCopypip install langchain-model-profiles\\nCopylangchain-profiles refresh --provider <provider> --data-dir <data_dir>\\nThis command:\\nDownloads the latest data for <provider> from models.dev\\nMerges augmentations from profile_augmentations.toml in <data_dir>\\nWrites merged profiles to profiles.py in <data_dir>\\nFor example: from libs/partners/anthropic in the LangChain monorepo:Copyuv run --with langchain-model-profiles --provider anthropic --data-dir langchain_anthropic/data\\n\\n\\u200bMultimodal\\nCertain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.\\nAll LangChain chat models with underlying multimodal capabilities support:\\nData in the cross-provider standard format (see our messages guide)\\nOpenAI chat completions format\\nAny format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)\\n\\nSee the multimodal section of the messages guide for details.\\nSome models can return multimodal data as part of their response. If invoked to do so, the resulting AIMessage will have content blocks with multimodal types.\\nMultimodal outputCopyresponse = model.invoke(\"Create a picture of a cat\")\\nprint(response.content_blocks)\\n# [\\n#     {\"type\": \"text\", \"text\": \"Here\\'s a picture of a cat\"},\\n#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\\n# ]\\n\\nSee the integrations page for details on specific providers.\\n\\u200bReasoning\\nMany models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.\\nIf supported by the underlying model, you can surface this reasoning process to better understand how the model arrived at its final answer.\\nStream reasoning outputComplete reasoning outputCopyfor chunk in model.stream(\"Why do parrots have colorful feathers?\"):\\n    reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\\n    print(reasoning_steps if reasoning_steps else chunk.text)\\n\\nDepending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical â€œtiersâ€ of reasoning (e.g., \\'low\\' or \\'high\\') or integer token budgets.\\nFor details, see the integrations page or reference for your respective chat model.\\n\\u200bLocal models\\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\\nOllama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page.\\n\\u200bPrompt caching\\nMany providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit:\\n\\nImplicit prompt caching: providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini.\\nExplicit caching: providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:\\n\\nChatOpenAI (via prompt_cache_key)\\nAnthropicâ€™s AnthropicPromptCachingMiddleware\\nGemini.\\nAWS Bedrock\\n\\n\\n\\nPrompt caching is often only engaged above a minimum input token threshold. See provider pages for details.\\nCache usage will be reflected in the usage metadata of the model response.\\n\\u200bServer-side tool use\\nSome providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\\nIf a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:\\nInvoke with server-side tool useCopyfrom langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(\"gpt-4.1-mini\")\\n\\ntool = {\"type\": \"web_search\"}\\nmodel_with_tools = model.bind_tools([tool])\\n\\nresponse = model_with_tools.invoke(\"What was a positive news story from today?\")\\nresponse.content_blocks\\n\\nResultCopy[\\n    {\\n        \"type\": \"server_tool_call\",\\n        \"name\": \"web_search\",\\n        \"args\": {\\n            \"query\": \"positive news stories today\",\\n            \"type\": \"search\"\\n        },\\n        \"id\": \"ws_abc123\"\\n    },\\n    {\\n        \"type\": \"server_tool_result\",\\n        \"tool_call_id\": \"ws_abc123\",\\n        \"status\": \"success\"\\n    },\\n    {\\n        \"type\": \"text\",\\n        \"text\": \"Here are some positive news stories from today...\",\\n        \"annotations\": [\\n            {\\n                \"end_index\": 410,\\n                \"start_index\": 337,\\n                \"title\": \"article title\",\\n                \"type\": \"citation\",\\n                \"url\": \"...\"\\n            }\\n        ]\\n    }\\n]\\nSee all 29 lines\\nThis represents a single conversational turn; there are no associated ToolMessage objects that need to be passed in as in client-side tool-calling.\\nSee the integration page for your given provider for available tools and usage details.\\n\\u200bRate limiting\\nMany chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\\nTo help manage rate limits, chat model integrations accept a rate_limiter parameter that can be provided during initialization to control the rate at which requests are made.\\nInitialize and use a rate limiterLangChain in comes with (an optional) built-in InMemoryRateLimiter. This limiter is thread safe and can be shared by multiple threads in the same process.Define a rate limiterCopyfrom langchain_core.rate_limiters import InMemoryRateLimiter\\n\\nrate_limiter = InMemoryRateLimiter(\\n    requests_per_second=0.1,  # 1 request every 10s\\n    check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\\n    max_bucket_size=10,  # Controls the maximum burst size.\\n)\\n\\nmodel = init_chat_model(\\n    model=\"gpt-5\",\\n    model_provider=\"openai\",\\n    rate_limiter=rate_limiter  \\n)\\nThe provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\\n\\u200bBase URL or proxy\\nFor many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.\\nBase URLMany model providers offer OpenAI-compatible APIs (e.g., Together AI, vLLM). You can use init_chat_model with these providers by specifying the appropriate base_url parameter:Copymodel = init_chat_model(\\n    model=\"MODEL_NAME\",\\n    model_provider=\"openai\",\\n    base_url=\"BASE_URL\",\\n    api_key=\"YOUR_API_KEY\",\\n)\\nWhen using direct chat model class instantiation, the parameter name may vary by provider. Check the respective reference for details.\\nProxy configurationFor deployments requiring HTTP proxies, some model integrations support proxy configuration:Copyfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(\\n    model=\"gpt-4o\",\\n    openai_proxy=\"http://proxy.example.com:8080\"\\n)\\nProxy support varies by integration. Check the specific model providerâ€™s reference for proxy configuration options.\\n\\u200bLog probabilities\\nCertain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the logprobs parameter when initializing the model:\\nCopymodel = init_chat_model(\\n    model=\"gpt-4o\",\\n    model_provider=\"openai\"\\n).bind(logprobs=True)\\n\\nresponse = model.invoke(\"Why do parrots talk?\")\\nprint(response.response_metadata[\"logprobs\"])\\n\\n\\u200bToken usage\\nA number of model providers return token usage information as part of the invocation response. When available, this information will be included on the AIMessage objects produced by the corresponding model. For more details, see the messages guide.\\nSome provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the streaming usage metadata section of the integration guide for details.\\nYou can track aggregate token counts across models in an application using either a callback or context manager, as shown below:\\n Callback handler Context managerCopyfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.callbacks import UsageMetadataCallbackHandler\\n\\nmodel_1 = init_chat_model(model=\"gpt-4o-mini\")\\nmodel_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\\n\\ncallback = UsageMetadataCallbackHandler()\\nresult_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\\nresult_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\\ncallback.usage_metadata\\nCopy{\\n    \\'gpt-4o-mini-2024-07-18\\': {\\n        \\'input_tokens\\': 8,\\n        \\'output_tokens\\': 10,\\n        \\'total_tokens\\': 18,\\n        \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0},\\n        \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}\\n    },\\n    \\'claude-haiku-4-5-20251001\\': {\\n        \\'input_tokens\\': 8,\\n        \\'output_tokens\\': 21,\\n        \\'total_tokens\\': 29,\\n        \\'input_token_details\\': {\\'cache_read\\': 0, \\'cache_creation\\': 0}\\n    }\\n}\\n\\n\\u200bInvocation config\\nWhen invoking a model, you can pass additional configuration through the config parameter using a RunnableConfig dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.\\nCommon configuration options include:\\nInvocation with configCopyresponse = model.invoke(\\n    \"Tell me a joke\",\\n    config={\\n        \"run_name\": \"joke_generation\",      # Custom name for this run\\n        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\\n        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\\n        \"callbacks\": [my_callback_handler], # Callback handlers\\n    }\\n)\\n\\nThese configuration values are particularly useful when:\\n\\nDebugging with LangSmith tracing\\nImplementing custom logging or monitoring\\nControlling resource usage in production\\nTracking invocations across complex pipelines\\n\\nKey configuration attributes\\u200brun_namestringIdentifies this specific invocation in logs and traces. Not inherited by sub-calls.\\u200btagsstring[]Labels inherited by all sub-calls for filtering and organization in debugging tools.\\u200bmetadataobjectCustom key-value pairs for tracking additional context, inherited by all sub-calls.\\u200bmax_concurrencynumberControls the maximum number of parallel calls when using batch() or batch_as_completed().\\u200bcallbacksarrayHandlers for monitoring and responding to events during execution.\\u200brecursion_limitnumberMaximum recursion depth for chains to prevent infinite loops in complex pipelines.\\nSee full RunnableConfig reference for all supported attributes.\\n\\u200bConfigurable models\\nYou can also create a runtime-configurable model by specifying configurable_fields. If you donâ€™t specify a model value, then \\'model\\' and \\'model_provider\\' will be configurable by default.\\nCopyfrom langchain.chat_models import init_chat_model\\n\\nconfigurable_model = init_chat_model(temperature=0)\\n\\nconfigurable_model.invoke(\\n    \"what\\'s your name\",\\n    config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\\n)\\nconfigurable_model.invoke(\\n    \"what\\'s your name\",\\n    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},  # Run with Claude\\n)\\n\\nConfigurable model with default valuesWe can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:Copyfirst_model = init_chat_model(\\n        model=\"gpt-4.1-mini\",\\n        temperature=0,\\n        configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\\n        config_prefix=\"first\",  # Useful when you have a chain with multiple models\\n)\\n\\nfirst_model.invoke(\"what\\'s your name\")\\nCopyfirst_model.invoke(\\n    \"what\\'s your name\",\\n    config={\\n        \"configurable\": {\\n            \"first_model\": \"claude-sonnet-4-5-20250929\",\\n            \"first_temperature\": 0.5,\\n            \"first_max_tokens\": 100,\\n        }\\n    },\\n)\\nSee the init_chat_model reference for more details on configurable_fields and config_prefix.\\nUsing a configurable model declarativelyWe can call declarative operations like bind_tools, with_structured_output, with_configurable, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.Copyfrom pydantic import BaseModel, Field\\n\\n\\nclass GetWeather(BaseModel):\\n    \"\"\"Get the current weather in a given location\"\"\"\\n\\n        location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\\n\\n\\nclass GetPopulation(BaseModel):\\n    \"\"\"Get the current population in a given location\"\"\"\\n\\n        location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\\n\\n\\nmodel = init_chat_model(temperature=0)\\nmodel_with_tools = model.bind_tools([GetWeather, GetPopulation])\\n\\nmodel_with_tools.invoke(\\n    \"what\\'s bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}}\\n).tool_calls\\nCopy[\\n    {\\n        \\'name\\': \\'GetPopulation\\',\\n        \\'args\\': {\\'location\\': \\'Los Angeles, CA\\'},\\n        \\'id\\': \\'call_Ga9m8FAArIyEjItHmztPYA22\\',\\n        \\'type\\': \\'tool_call\\'\\n    },\\n    {\\n        \\'name\\': \\'GetPopulation\\',\\n        \\'args\\': {\\'location\\': \\'New York, NY\\'},\\n        \\'id\\': \\'call_jh2dEvBaAHRaw5JUDthOs7rt\\',\\n        \\'type\\': \\'tool_call\\'\\n    }\\n]\\nCopymodel_with_tools.invoke(\\n    \"what\\'s bigger in 2024 LA or NYC\",\\n    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},\\n).tool_calls\\nCopy[\\n    {\\n        \\'name\\': \\'GetPopulation\\',\\n        \\'args\\': {\\'location\\': \\'Los Angeles, CA\\'},\\n        \\'id\\': \\'toolu_01JMufPf4F4t2zLj7miFeqXp\\',\\n        \\'type\\': \\'tool_call\\'\\n    },\\n    {\\n        \\'name\\': \\'GetPopulation\\',\\n        \\'args\\': {\\'location\\': \\'New York City, NY\\'},\\n        \\'id\\': \\'toolu_01RQBHcE8kEEbYTuuS8WqY1u\\',\\n        \\'type\\': \\'tool_call\\'\\n    }\\n]\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoAgentsPreviousMessagesNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls=[\n",
    "    \"https://docs.langchain.com/\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/models\"\n",
    "]\n",
    "docs=[WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dae2380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='ea476bd2-9b58-4352-adb2-178e468b37d6', metadata={'source': 'https://docs.langchain.com/', 'title': 'Home - Docs by LangChain', 'language': 'en'}, page_content=\"Home - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageHomeSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationDocumentationLangChain is the platform for agent engineering. AI teams at Replit, Clay, Rippling, Cloudflare, Workday, and more trust LangChainâ€™s products to engineer reliable agents.Open source agent frameworks Python TypeScriptLangChain (Python)Quickly get started building agents, with any model provider of your choice.Learn moreLangGraph (Python)Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.Learn moreDeep Agents (Python)Build agents that can tackle complex, multi-step tasks.Learn moreLangSmithLangSmith is a platform that helps AI teams use live production data for continuous testing and improvement. LangSmith provides:ObservabilitySee exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.Learn moreEvaluationTest and score agent behavior on production data or offline datasets to continuously improve performance.Learn morePrompt EngineeringIterate on prompts with version control, prompt optimization, and collaboration features.Learn moreDeploymentShip your agent in one click, using scalable infrastructure built for long-running tasks.Learn moreAgent Builder (Beta)Turn natural-language ideas into production agents with persistent memory and self-updating capabilities.Learn moreLangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.Get startedBuild your first agent with LangChainGet startedSign up for LangSmithTry LangSmithBuild an advanced agent with LangGraphGet startedEnroll in LangChain AcademyGet startedAdditional resourcesCommunity forumAsk questions, share solutions, and discuss best practices.JoinCommunity SlackConnect with other builders and get quick help.JoinSupport portalSubmit tickets and track support requests.VisitLangSmith statusReal-time status of LangSmith services and APIs.View\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\"),\n",
       " Document(id='eccb80a9-fff9-4ad5-8f62-1a07eeab9280', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Create an agent\\nCopy# pip install -qU langchain \"langchain[anthropic]\"\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nSee the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChainâ€™s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChainâ€™s agents are built on top of LangGraph. This allows us to take advantage of LangGraphâ€™s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangChainNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(id='0d102cf8-d8dd-45a2-8d97-69e05593170d', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/models', 'title': 'Models - Docs by LangChain', 'language': 'en'}, page_content='Models - Docs by LangChainSkip to main contentðŸš€ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsModelsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageBasic usageInitialize a modelKey methodsParametersInvocationInvokeStreamBatchTool callingStructured outputSupported modelsAdvanced topicsModel profilesMultimodalReasoningLocal modelsPrompt cachingServer-side tool useRate limitingBase URL or proxyLog probabilitiesToken usageInvocation configConfigurable modelsCore componentsModelsCopy pageCopy pageLLMs are powerful AI tools that can interpret and generate text like humans. Theyâ€™re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\\nIn addition to text generation, many models support:\\n\\n Tool calling - calling external tools (like databases queries or API calls) and use results in their responses.\\n Structured output - where the modelâ€™s response is constrained to follow a defined format.\\n Multimodality - process and return data other than text, such as images, audio, and video.\\n Reasoning - models perform multi-step reasoning to arrive at a conclusion.\\n\\nModels are the reasoning engine of agents. They drive the agentâ€™s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\\nThe quality and capabilities of the model you choose directly impact your agentâ€™s baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\\nLangChainâ€™s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\\nFor provider-specific integration information and capabilities, see the providerâ€™s chat model page.\\n\\u200bBasic usage\\nModels can be utilized in two ways:\\n\\nWith agents - Models can be dynamically specified when creating an agent.\\nStandalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\\n\\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\\n\\u200bInitialize a model\\nThe easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below):\\n OpenAI Anthropic Azure Google Gemini AWS BedrockðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nCopyresponse = model.invoke(\"Why do parrots talk?\")'),\n",
       " Document(id='0b8f5795-1112-4a48-826a-559000eb1812', metadata={'source': 'https://docs.langchain.com/oss/python/langchain/models', 'title': 'Models - Docs by LangChain', 'language': 'en'}, page_content='elif event[\"event\"] == \"on_chat_model_stream\":\\n        print(f\"Token: {event[\\'data\\'][\\'chunk\\'].text}\")\\n\\n    elif event[\"event\"] == \"on_chat_model_end\":\\n        print(f\"Full message: {event[\\'data\\'][\\'output\\'].text}\")\\n\\n    else:\\n        pass\\nCopyInput: Hello\\nToken: Hi\\nToken:  there\\nToken: !\\nToken:  How\\nToken:  can\\nToken:  I\\n...\\nFull message: Hi there! How can I help today?\\nSee the astream_events() reference for event types and other details.\"Auto-streaming\" chat modelsLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.In LangGraph agents, for example, you can call model.invoke() within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\\u200bHow it worksWhen you invoke() a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking on_llm_new_token events in LangChainâ€™s callback system.Callback events allow LangGraph stream() and astream_events() to surface the chat modelâ€™s output in real-time.\\n\\u200bBatch\\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\\nBatchCopyresponses = model.batch([\\n    \"Why do parrots have colorful feathers?\",\\n    \"How do airplanes fly?\",\\n    \"What is quantum computing?\"\\n])\\nfor response in responses:\\n    print(response)\\n\\nThis section describes a chat model method batch(), which parallelizes model calls client-side.It is distinct from batch APIs supported by inference providers, such as OpenAI or Anthropic.\\nBy default, batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed():\\nYield batch responses upon completionCopyfor response in model.batch_as_completed([\\n    \"Why do parrots have colorful feathers?\",\\n    \"How do airplanes fly?\",\\n    \"What is quantum computing?\"\\n]):\\n    print(response)\\n\\nWhen using batch_as_completed(), results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\\nWhen processing a large number of inputs using batch() or batch_as_completed(), you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary.Batch with max concurrencyCopymodel.batch(\\n    list_of_inputs,\\n    config={\\n        \\'max_concurrency\\': 5,  # Limit to 5 parallel calls\\n    }\\n)\\nSee the RunnableConfig reference for a full list of supported attributes.\\nFor more details on batching, see the reference.\\n\\n\\u200bTool calling\\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\\n\\nA schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\\nA function or coroutine to execute.\\n\\nYou may hear the term â€œfunction callingâ€. We use this interchangeably with â€œtool callingâ€.\\nHereâ€™s the basic tool calling flow between a user and a model:')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list=[item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "doc_split=text_splitter.split_documents(doc_list)\n",
    "\n",
    "langchain_vectorstore=FAISS.from_documents(documents=doc_split, embedding=OpenAIEmbeddings())\n",
    "langchain_retriever=langchain_vectorstore.as_retriever()\n",
    "langchain_retriever.invoke(\"What is Langchain?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f2855e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever to Retriever Tools\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain=create_retriever_tool(\n",
    "    langchain_retriever,\n",
    "    \"langchain_retriever_vector_db_blog\",\n",
    "    \"Search for information about LangGraph. For any questions about LangGraph, you must use this tool!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0513361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='langchain_retriever_vector_db_blog', description='Search for information about LangGraph. For any questions about LangGraph, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000221F55DA050>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C12C0CA0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000221C08F37F0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C12C0CA0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d73bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool, retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "818bcad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='retriever_vector_db_blog', description='Search for information about LangGraph. For any questions about LangGraph, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000221F55DA050>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C1297DF0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000221C08F37F0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C1297DF0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')),\n",
       " Tool(name='langchain_retriever_vector_db_blog', description='Search for information about LangGraph. For any questions about LangGraph, you must use this tool!', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000221F55DA050>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C12C0CA0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000221C08F37F0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000221C12C0CA0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
