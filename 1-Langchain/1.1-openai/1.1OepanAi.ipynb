{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f443f9",
   "metadata": {},
   "source": [
    "## OpenAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1052771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc30f15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x000001C2F04D3D30> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001C2F04D3B20> root_client=<openai.OpenAI object at 0x000001C2CF50E290> root_async_client=<openai.AsyncOpenAI object at 0x000001C2F04D0430> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********') stream_usage=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0854d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response from LLM\n",
    "data=llm.invoke(\"What is RAG in AI ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb54ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"RAG in AI stands for Retrieval-Augmented Generation. It is a framework that combines retrieval-based models with generative models to improve the quality and accuracy of text generation tasks. The key idea behind RAG is to enhance the generative capabilities of language models by incorporating relevant information retrieved from a large corpus of documents or a knowledge base.\\n\\nHere's how it typically works:\\n\\n1. **Retrieval**: Given an input query, a retrieval model searches through a vast database or document store to find relevant documents or pieces of information. This step is critical because it provides the generative model with contextually relevant data that may not be part of its training.\\n\\n2. **Augmentation**: The retrieved information is then used to augment the input. This means that the generative model receives not only the original input query but also the supplemental information distilled from the retrieved documents.\\n\\n3. **Generation**: Using the augmented input, a generative language model (such as a transformer-based model like GPT) generates a response or output text. By leveraging the additional context, the output is generally more accurate and contextually appropriate.\\n\\nRAG is particularly useful in applications like question answering, where specific and accurate information retrieval is necessary to complement the generative capabilities of the model. It helps in dealing with the limitations of language models that might not have all the necessary information encoded in their parameters. Overall, RAG represents a synergistic approach that leverages both retrieval and generation to improve performance on complex language tasks.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 299, 'prompt_tokens': 14, 'total_tokens': 313, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_689bad8e9a', 'id': 'chatcmpl-CgoZUrmdFDZbxHumd1RcGIFB99nk4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--c7b3b255-f675-4ac4-9d42-96174f2de870-0' usage_metadata={'input_tokens': 14, 'output_tokens': 299, 'total_tokens': 313, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12e207eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Please response to the user queries'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question:{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful AI assistant. Please response to the user queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b65bb27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Machine learning is a subset of artificial intelligence (AI) that involves the development of algorithms and statistical models that enable computers to perform specific tasks without being explicitly programmed to do so. At its core, machine learning focuses on using data-driven approaches to improve the performance of a system over time as it is exposed to more data.\\n\\nThere are several types of machine learning, including:\\n\\n1. **Supervised Learning**: In this type of learning, the algorithm is trained on a labeled dataset, which means that each training example is paired with an output label. The model learns to make predictions or decisions based on this data.\\n\\n2. **Unsupervised Learning**: Here, the model is trained on data without any labels, meaning it tries to identify patterns or structures in the input data. Common techniques include clustering and association.\\n\\n3. **Semi-supervised Learning**: This approach is a combination of supervised and unsupervised learning. It uses a small amount of labeled data along with a larger amount of unlabeled data to improve learning accuracy.\\n\\n4. **Reinforcement Learning**: In this type, an agent learns to make decisions by taking actions in an environment to achieve maximum cumulative reward. It is often used in areas like robotics, gaming, and autonomous vehicles.\\n\\n5. **Deep Learning**: A specialized form of machine learning that involves neural networks with many layers (hence \"deep\") and is particularly effective in tasks like image and speech recognition.\\n\\nMachine learning is widely used in various applications such as recommendation systems, predictive modeling, computer vision, natural language processing, and more. It enables systems to improve their performance and make data-driven decisions.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 331, 'prompt_tokens': 31, 'total_tokens': 362, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_689bad8e9a', 'id': 'chatcmpl-CgozKRwE71qy2uLhTrIe842XdbMEY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--70ef2624-dbde-43b1-bd1e-51a5c6fa90e6-0' usage_metadata={'input_tokens': 31, 'output_tokens': 331, 'total_tokens': 362, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm\n",
    "result=chain.invoke({\"question\":\"What is Machine Learning ?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fd357ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subset of machine learning and artificial intelligence (AI) that mimics the workings of the human brain in processing data and creating patterns for use in decision making. It is particularly effective in handling large volumes of unstructured data like images, audio, and text.\n",
      "\n",
      "At its core, deep learning utilizes neural networks with many layers (hence \"deep\") to model complex patterns and representations in data. These neural networks consist of layers of interconnected nodes, or \"neurons,\" where each connection has an associated weight that is adjusted during the training process to minimize error.\n",
      "\n",
      "Deep learning has led to breakthroughs in various fields, including computer vision, natural language processing, and speech recognition, thanks to its ability to automatically extract high-level features from raw input data without the need for manual feature engineering. Techniques like convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformer networks are typical architectures used in deep learning applications.\n"
     ]
    }
   ],
   "source": [
    "### Parsers\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "result=chain.invoke({\"question\":\"What is Deep Learning ?\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6dbb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
